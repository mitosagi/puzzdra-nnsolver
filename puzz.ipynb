{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "puzz.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaB5QqT4b8Spt7GXJZ6ble",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitosagi/puzzdra-nnsolver/blob/master/puzz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrk7ju8ZSpq1"
      },
      "source": [
        "## 初期化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-syCb8S5ijR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcbb7b4-98b2-4646-d850-ed67f8f1f55f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MinijqIGMKzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da61e11-4b06-483c-b324-431154c45436"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/User/python/puzzdra-nnsolver /content/puzzdra-nnsolver\n",
        "%cd /content/puzzdra-nnsolver\n",
        "!pip install --log=pip_log -e .\n",
        "!python puzz_test.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/puzzdra-nnsolver\n",
            "Obtaining file:///content/puzzdra-nnsolver\n",
            "Installing collected packages: Puzzpy\n",
            "  Running setup.py develop for Puzzpy\n",
            "Successfully installed Puzzpy\n",
            "163663\n",
            "334311\n",
            "323365\n",
            "136624\n",
            "145342\n",
            "[[1, 3, 6, 6, 6, 3], [3, 3, 4, 3, 1, 1], [3, 2, 3, 3, 6, 5], [1, 3, 6, 6, 2, 4], [1, 4, 5, 3, 4, 2]]\n",
            "[[0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
            "[[127, 6, 3, 6, 6, 3], [3, 3, 4, 3, 1, 1], [3, 2, 3, 3, 6, 5], [1, 3, 6, 6, 2, 4], [1, 4, 5, 3, 4, 2]]\n",
            "[[0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
            "[[1, 6, 4, 6, 6, 3], [3, 3, 3, 3, 1, 1], [3, 2, 3, 3, 6, 5], [1, 3, 6, 6, 2, 4], [1, 4, 5, 3, 4, 2]]\n",
            "[[0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
            "[[1, 6, 6, 3, 6, 3], [3, 3, 4, 3, 1, 1], [3, 2, 3, 3, 6, 5], [1, 3, 6, 6, 2, 4], [1, 4, 5, 3, 4, 2]]\n",
            "[[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
            "10\n",
            "453132\n",
            "566644\n",
            "565226\n",
            "525664\n",
            "462244\n",
            "<class 'int'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3eBhEr5u8xf",
        "outputId": "4b235fc6-495b-42e4-a1e5-c2e6e76d2f48"
      },
      "source": [
        "!pip install git+https://github.com/DLR-RM/stable-baselines3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/DLR-RM/stable-baselines3\n",
            "  Cloning https://github.com/DLR-RM/stable-baselines3 to /tmp/pip-req-build-u2dh0r08\n",
            "  Running command git clone -q https://github.com/DLR-RM/stable-baselines3 /tmp/pip-req-build-u2dh0r08\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.1.0a11) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.1.0a11) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.1.0a11) (1.9.0+cu102)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.1.0a11) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.1.0a11) (1.1.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.1.0a11) (3.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3==1.1.0a11) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3==1.1.0a11) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3==1.1.0a11) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3==1.1.0a11) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3==1.1.0a11) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.1.0a11) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.1.0a11) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.1.0a11) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3==1.1.0a11) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->stable-baselines3==1.1.0a11) (1.15.0)\n",
            "Building wheels for collected packages: stable-baselines3\n",
            "  Building wheel for stable-baselines3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stable-baselines3: filename=stable_baselines3-1.1.0a11-cp37-none-any.whl size=160811 sha256=c2bc51420182d388351cd2f053279b59c9edff2f0d6e9ee871d06fcd42e09344\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bk4nax9u/wheels/cf/89/6b/cd4b89427eb5ff0858bcba73911088d606c59eb3a97290b1bb\n",
            "Successfully built stable-baselines3\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-1.1.0a11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivAHcGIGSmEM"
      },
      "source": [
        "## サンプルの実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwK7sQOIOe67"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Gymのインターフェースに従うカスタム環境\n",
        "  エージェントが常に左に行くことを学ぶ環境\n",
        "  \"\"\"\n",
        "  # ColabのためGUIを実装できない\n",
        "  metadata = {'render.modes': ['console']}\n",
        "\n",
        "  # 定数を定義\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoLeftEnv, self).__init__()\n",
        "\n",
        "    # 1Dグリッドのサイズ\n",
        "    self.grid_size = grid_size\n",
        "\n",
        "    # グリッドの右側でエージェントを初期化\n",
        "    self.agent_pos = grid_size - 1\n",
        "\n",
        "    # 行動空間と状態空間を定義\n",
        "    # gym.spacesオブジェクトでなければならない\n",
        "    # 離散行動を使用する場合の例には、左と右の2つがある\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "\n",
        "    # 状態はエージェントの座標になる\n",
        "    # Discrete空間とBox空間の両方で表現できる\n",
        "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
        "                                       shape=(1,), dtype=np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    【重要】観測はnumpy配列でなければならない\n",
        "    :return: (np.array)\n",
        "    \"\"\"\n",
        "    # グリッドの右側でエージェントを初期化\n",
        "    self.agent_pos = self.grid_size - 1\n",
        "\n",
        "    # float32に変換してより一般的なものにします（連続行動を使用する場合）\n",
        "    return np.array(self.agent_pos).astype(np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == self.LEFT:\n",
        "      self.agent_pos -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      self.agent_pos += 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "\n",
        "    # グリッドの境界を表現\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "    # グリッドの左側にいるか\n",
        "    done = self.agent_pos == 0\n",
        "\n",
        "    # ゴールを除くすべての場所で0の報酬\n",
        "    reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "    # 必要に応じて情報を渡すことができるが、現在は未使用\n",
        "    info = {}\n",
        "\n",
        "    return np.array(self.agent_pos).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console', close=False):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "\n",
        "    # エージェントは「x」、残りは「.」として表現\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_pos))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlrpB6VxS_Z0",
        "outputId": "e373e7e7-1419-4c32-da4c-a4e20cc92055"
      },
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "GO_LEFT = 0\n",
        "\n",
        "# ハードコードされた最高のエージェント：常に左に行く\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(GO_LEFT)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "1\n",
            "Step 1\n",
            "obs= 8.0 reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= 7.0 reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= 6.0 reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= 5.0 reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= 4.0 reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= 3.0 reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= 2.0 reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= 1.0 reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= 0.0 reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIbo58d5TBwn"
      },
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# 環境の生成\n",
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "# 環境のラップ\n",
        "env = Monitor(env, filename=None, allow_early_resets=True)\n",
        "env = DummyVecEnv([lambda: env])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTgpCUYXTPis",
        "outputId": "b12ceb9c-9059-4415-a69b-8326e1f5d310"
      },
      "source": [
        "# エージェントの訓練\n",
        "model = PPO('MlpPolicy', env, verbose=1).learn(5000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 92.6     |\n",
            "|    ep_rew_mean     | 1        |\n",
            "| time/              |          |\n",
            "|    fps             | 935      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51.4        |\n",
            "|    ep_rew_mean          | 1           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 711         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020174187 |\n",
            "|    clip_fraction        | 0.412       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.675      |\n",
            "|    explained_variance   | 0.251       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00652     |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0441     |\n",
            "|    value_loss           | 0.0448      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 21.1       |\n",
            "|    ep_rew_mean          | 1          |\n",
            "| time/                   |            |\n",
            "|    fps                  | 677        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 9          |\n",
            "|    total_timesteps      | 6144       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02556822 |\n",
            "|    clip_fraction        | 0.452      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.609     |\n",
            "|    explained_variance   | 0.517      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0265    |\n",
            "|    n_updates            | 20         |\n",
            "|    policy_gradient_loss | -0.0557    |\n",
            "|    value_loss           | 0.0365     |\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR69cZGVTQ00",
        "outputId": "e5c908a1-d8dc-4682-8d9b-8b5a012dd344"
      },
      "source": [
        "# 訓練済みエージェントのテスト\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # VecEnvは、エピソード完了に遭遇すると自動的にリセットされることに注意\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "Action:  [0]\n",
            "obs= [[8.]] reward= [0.] done= [False]\n",
            "........x..\n",
            "Step 2\n",
            "Action:  [0]\n",
            "obs= [[7.]] reward= [0.] done= [False]\n",
            ".......x...\n",
            "Step 3\n",
            "Action:  [0]\n",
            "obs= [[6.]] reward= [0.] done= [False]\n",
            "......x....\n",
            "Step 4\n",
            "Action:  [0]\n",
            "obs= [[5.]] reward= [0.] done= [False]\n",
            ".....x.....\n",
            "Step 5\n",
            "Action:  [0]\n",
            "obs= [[4.]] reward= [0.] done= [False]\n",
            "....x......\n",
            "Step 6\n",
            "Action:  [0]\n",
            "obs= [[3.]] reward= [0.] done= [False]\n",
            "...x.......\n",
            "Step 7\n",
            "Action:  [0]\n",
            "obs= [[2.]] reward= [0.] done= [False]\n",
            "..x........\n",
            "Step 8\n",
            "Action:  [0]\n",
            "obs= [[1.]] reward= [0.] done= [False]\n",
            ".x.........\n",
            "Step 9\n",
            "Action:  [0]\n",
            "obs= [[9.]] reward= [1.] done= [ True]\n",
            ".........x.\n",
            "Goal reached! reward= [1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdUISRWiX4kM"
      },
      "source": [
        "## 実際の処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI2D4A-cXnQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921acb9c-bb81-46ef-9e36-cda93015b852"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "from puzzpy import PuzzTable\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "class PuzzEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  パズドラの環境\n",
        "  \"\"\"\n",
        "  # ColabのためGUIを実装できない\n",
        "  metadata = {'render.modes': ['console']}\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PuzzEnv, self).__init__()\n",
        "\n",
        "    self.action_space = spaces.Discrete(5)\n",
        "\n",
        "    # 状態はエージェントの座標になる\n",
        "    # Discrete空間とBox空間の両方で表現できる\n",
        "    # self.observation_space = spaces.Dict({'table': spaces.Box(low=0, high=5,\n",
        "    #                                    shape=(5,6), dtype=np.float32),\n",
        "    #                                    'start': spaces.Discrete(30),\n",
        "    #                                    'turn': spaces.Box(low=1, high=155,\n",
        "    #                                    shape=(1,), dtype=np.float32),\n",
        "    #                                    'rew': spaces.Box(low=1, high=100,\n",
        "    #                                    shape=(1,), dtype=np.float32)})\n",
        "    \n",
        "    #                                    'start': spaces.Box(low=0, high=1,\n",
        "    #                                    shape=(5,6), dtype=np.float32),\n",
        "    self.observation_space = spaces.Box(low=0, high=255, shape=(3,5,6), dtype=np.uint8)\n",
        "\n",
        "  def zerosturn(self, table):\n",
        "    zeros = np.zeros((5,6))\n",
        "    zeros[0][0] = table.get_turn()\n",
        "    zeros[0][1] = table.eval_otoshi()\n",
        "    return zeros\n",
        "\n",
        "  def retobs(self, table):\n",
        "\n",
        "    \n",
        "    # return {'table': np.array(table.get_table()).astype(np.float32),\n",
        "    #           'start': np.array(table.get_XY_as_table()).astype(np.float32),\n",
        "    #           'turn': table.get_turn(),'rew': table.eval_otoshi()}, table.eval_otoshi(), True, {}\n",
        "    # return {'table': np.array(table.get_table()).astype(np.float32),\n",
        "    #           'start': np.flatnonzero(table.get_XY_as_table())[0],\n",
        "    #           'turn': table.get_turn(),'rew': table.eval_otoshi()}, table.eval_otoshi(), True, {}\n",
        "    return np.stack([np.array(table.get_table()).astype(np.uint8),\n",
        "                     np.array(table.get_XY_as_table()).astype(np.uint8),\n",
        "                     self.zerosturn(table)]), table.eval_otoshi() - self.initreward, True, {}\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    【重要】観測はnumpy配列でなければならない\n",
        "    :return: (np.array)\n",
        "    \"\"\"\n",
        "    self.table = PuzzTable(155)\n",
        "    self.initreward = self.table.eval_otoshi()\n",
        "    return self.retobs(self.table)[0]\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == 4:\n",
        "      return self.retobs(self.table)\n",
        "\n",
        "    next_table = self.table.next_tables()[action]\n",
        "\n",
        "    if next_table.get_table()[0][0] == 127:\n",
        "      return self.retobs(self.table)\n",
        "\n",
        "    self.table = next_table\n",
        "\n",
        "    if self.table.get_turn() <= 0:\n",
        "      return self.retobs(self.table)\n",
        "\n",
        "    # return {'table': np.array(self.table.get_table()).astype(np.float32),\n",
        "    #           'start': np.array(self.table.get_XY_as_table()).astype(np.float32),\n",
        "    #           'turn': self.table.get_turn(),'rew': self.table.eval_otoshi()}, 0, False, {}\n",
        "    # return {'table': np.array(self.table.get_table()).astype(np.float32),\n",
        "    #           'start': np.flatnonzero(table.get_XY_as_table())[0],\n",
        "    #           'turn': self.table.get_turn(),'rew': self.table.eval_otoshi()}, 0, False, {}\n",
        "    return np.stack([np.array(self.table.get_table()).astype(np.uint8),\n",
        "                     np.array(self.table.get_XY_as_table()).astype(np.uint8),\n",
        "                     self.zerosturn(self.table)]), 0, False, {}\n",
        "\n",
        "  def render(self, mode='console', close=False):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "\n",
        "    start = self.table.get_XY_as_table()\n",
        "    table = self.table.get_table()\n",
        "    for i in range(5):\n",
        "      for j in range(6):\n",
        "        if start[i][j] == 1:\n",
        "          print(bcolors.FAIL + str(table[i][j]) + bcolors.ENDC, end='')\n",
        "        else:\n",
        "          print(table[i][j], end='')\n",
        "\n",
        "      print('')\n",
        "    # print('\\n'.join([''.join(['{:2}'.format(item) for item in row]) \n",
        "    #   for row in self.table.get_table()]))\n",
        "    # for i in self.table.get_table():\n",
        "    #   print(*i)\n",
        "\n",
        "check_env(PuzzEnv())"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/env_checker.py:48: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom feature extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
            "  \"The minimal resolution for an image is 36x36 for the default `CnnPolicy`. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Z9SliEfKci",
        "outputId": "77c07b0c-3af7-4fa7-b904-f17d5c3c1b2f"
      },
      "source": [
        "env = PuzzEnv()\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(env.action_space.sample())\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\u001b[91m6\u001b[0m425\n",
            "151664\n",
            "534212\n",
            "545342\n",
            "111455\n",
            "Box(0, 255, (3, 5, 6), uint8)\n",
            "Discrete(5)\n",
            "0\n",
            "Step 1\n",
            "obs= [[[  2.   6.   1.   4.   2.   5.]\n",
            "  [  1.   5.   6.   6.   6.   4.]\n",
            "  [  5.   3.   4.   2.   1.   2.]\n",
            "  [  5.   4.   5.   3.   4.   2.]\n",
            "  [  1.   1.   1.   4.   5.   5.]]\n",
            "\n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   1.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]]\n",
            "\n",
            " [[154.   2.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]]] reward= 0 done= False\n",
            "261425\n",
            "15\u001b[91m6\u001b[0m664\n",
            "534212\n",
            "545342\n",
            "111455\n",
            "Step 2\n",
            "obs= [[[  2.   6.   1.   4.   2.   5.]\n",
            "  [  1.   6.   5.   6.   6.   4.]\n",
            "  [  5.   3.   4.   2.   1.   2.]\n",
            "  [  5.   4.   5.   3.   4.   2.]\n",
            "  [  1.   1.   1.   4.   5.   5.]]\n",
            "\n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   1.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]]\n",
            "\n",
            " [[153.   1.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]]] reward= 0 done= False\n",
            "261425\n",
            "1\u001b[91m6\u001b[0m5664\n",
            "534212\n",
            "545342\n",
            "111455\n",
            "Step 3\n",
            "obs= [[[  2.   6.   1.   4.   2.   5.]\n",
            "  [  1.   6.   5.   6.   6.   4.]\n",
            "  [  5.   3.   4.   2.   1.   2.]\n",
            "  [  5.   4.   5.   3.   4.   2.]\n",
            "  [  1.   1.   1.   4.   5.   5.]]\n",
            "\n",
            " [[  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   1.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]]\n",
            "\n",
            " [[153.   1.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.]]] reward= -1 done= True\n",
            "261425\n",
            "1\u001b[91m6\u001b[0m5664\n",
            "534212\n",
            "545342\n",
            "111455\n",
            "Goal reached! reward= -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiOiIdZ_gvP3"
      },
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.envs.multi_input_envs import SimpleMultiObsEnv\n",
        "\n",
        "# 環境の生成\n",
        "env = PuzzEnv()\n",
        "\n",
        "# 環境のラップ\n",
        "env = Monitor(env, filename=None, allow_early_resets=True)\n",
        "# env = DummyVecEnv([lambda: env])\n",
        "# env = SimpleMultiObsEnv()"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myZbn2IRa9e8"
      },
      "source": [
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.type_aliases import Schedule\n",
        "from stable_baselines3.common.preprocessing import get_flattened_obs_dim, is_image_space\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "import torch as th\n",
        "from torch import nn\n",
        "\n",
        "class NatureCNN(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "    :param observation_space:\n",
        "    :param features_dim: Number of features extracted.\n",
        "        This corresponds to the number of unit for the last layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n",
        "        super(NatureCNN, self).__init__(observation_space, features_dim)\n",
        "        # We assume CxHxW images (channels first)\n",
        "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
        "        assert is_image_space(observation_space, check_channels=False), (\n",
        "            \"You should use NatureCNN \"\n",
        "            f\"only with images not with {observation_space}\\n\"\n",
        "            \"(you are probably using `CnnPolicy` instead of `MlpPolicy` or `MultiInputPolicy`)\\n\"\n",
        "            \"If you are using a custom environment,\\n\"\n",
        "            \"please check it using our env checker:\\n\"\n",
        "            \"https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html\"\n",
        "        )\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "class MyActorCriticCnnPolicy(ActorCriticPolicy):\n",
        "    \"\"\"\n",
        "    CNN policy class for actor-critic algorithms (has both policy and value prediction).\n",
        "    Used by A2C, PPO and the likes.\n",
        "    :param observation_space: Observation space\n",
        "    :param action_space: Action space\n",
        "    :param lr_schedule: Learning rate schedule (could be constant)\n",
        "    :param net_arch: The specification of the policy and value networks.\n",
        "    :param activation_fn: Activation function\n",
        "    :param ortho_init: Whether to use or not orthogonal initialization\n",
        "    :param use_sde: Whether to use State Dependent Exploration or not\n",
        "    :param log_std_init: Initial value for the log standard deviation\n",
        "    :param full_std: Whether to use (n_features x n_actions) parameters\n",
        "        for the std instead of only (n_features,) when using gSDE\n",
        "    :param sde_net_arch: Network architecture for extracting features\n",
        "        when using gSDE. If None, the latent features from the policy will be used.\n",
        "        Pass an empty list to use the states as features.\n",
        "    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure\n",
        "        a positive standard deviation (cf paper). It allows to keep variance\n",
        "        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n",
        "    :param squash_output: Whether to squash the output using a tanh function,\n",
        "        this allows to ensure boundaries when using gSDE.\n",
        "    :param features_extractor_class: Features extractor to use.\n",
        "    :param features_extractor_kwargs: Keyword arguments\n",
        "        to pass to the features extractor.\n",
        "    :param normalize_images: Whether to normalize images or not,\n",
        "         dividing by 255.0 (True by default)\n",
        "    :param optimizer_class: The optimizer to use,\n",
        "        ``th.optim.Adam`` by default\n",
        "    :param optimizer_kwargs: Additional keyword arguments,\n",
        "        excluding the learning rate, to pass to the optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: gym.spaces.Space,\n",
        "        action_space: gym.spaces.Space,\n",
        "        lr_schedule: Schedule,\n",
        "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
        "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
        "        ortho_init: bool = True,\n",
        "        use_sde: bool = False,\n",
        "        log_std_init: float = 0.0,\n",
        "        full_std: bool = True,\n",
        "        sde_net_arch: Optional[List[int]] = None,\n",
        "        use_expln: bool = False,\n",
        "        squash_output: bool = False,\n",
        "        features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n",
        "        features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        normalize_images: bool = True,\n",
        "        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n",
        "        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    ):\n",
        "        super(MyActorCriticCnnPolicy, self).__init__(\n",
        "            observation_space,\n",
        "            action_space,\n",
        "            lr_schedule,\n",
        "            net_arch,\n",
        "            activation_fn,\n",
        "            ortho_init,\n",
        "            use_sde,\n",
        "            log_std_init,\n",
        "            full_std,\n",
        "            sde_net_arch,\n",
        "            use_expln,\n",
        "            squash_output,\n",
        "            features_extractor_class,\n",
        "            features_extractor_kwargs,\n",
        "            normalize_images,\n",
        "            optimizer_class,\n",
        "            optimizer_kwargs,\n",
        "        )"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_qAMMmQnTtw",
        "outputId": "c0f4b61f-f2d8-4222-ccd0-c2c0ec1a1db8"
      },
      "source": [
        "# エージェントの訓練\n",
        "model = PPO(MyActorCriticCnnPolicy, env, verbose=1).learn(1600000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 3.1      |\n",
            "|    ep_rew_mean     | -0.06    |\n",
            "| time/              |          |\n",
            "|    fps             | 724      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 2.95        |\n",
            "|    ep_rew_mean          | -0.01       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 541         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013747014 |\n",
            "|    clip_fraction        | 0.0332      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | -0.000342   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.218       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00244    |\n",
            "|    value_loss           | 0.347       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 2.71        |\n",
            "|    ep_rew_mean          | 0           |\n",
            "| time/                   |             |\n",
            "|    fps                  | 497         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011591693 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.57       |\n",
            "|    explained_variance   | 0.000435    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.19        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00588    |\n",
            "|    value_loss           | 0.289       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 2.97         |\n",
            "|    ep_rew_mean          | 0.03         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 479          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0087068975 |\n",
            "|    clip_fraction        | 0.0306       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.54        |\n",
            "|    explained_variance   | 0.00218      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.0665       |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.000942    |\n",
            "|    value_loss           | 0.225        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.63        |\n",
            "|    ep_rew_mean          | -0.01       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 469         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 21          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009686651 |\n",
            "|    clip_fraction        | 0.0257      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.55       |\n",
            "|    explained_variance   | -0.0156     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0779      |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00184    |\n",
            "|    value_loss           | 0.248       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 3.16        |\n",
            "|    ep_rew_mean          | -0.03       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 463         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006431901 |\n",
            "|    clip_fraction        | 0.0255      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.00107     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.166       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00031    |\n",
            "|    value_loss           | 0.266       |\n",
            "-----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBVijrNbnUjI",
        "outputId": "d1b8ba24-9b32-458c-f687-a4f406b95127"
      },
      "source": [
        "# 訓練済みエージェントのテスト\n",
        "obs = env.reset()\n",
        "n_steps = 155\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  # print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  print('score=', obs[2][0][1])\n",
        "  env.render('console')\n",
        "  if done:\n",
        "    # VecEnvは、エピソード完了に遭遇すると自動的にリセットされることに注意\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "Action:  2\n",
            "score= 1.0\n",
            "621426\n",
            "256246\n",
            "33\u001b[91m3\u001b[0m241\n",
            "131433\n",
            "625431\n",
            "Step 2\n",
            "Action:  2\n",
            "score= 0.0\n",
            "621426\n",
            "256246\n",
            "331241\n",
            "13\u001b[91m3\u001b[0m433\n",
            "625431\n",
            "Step 3\n",
            "Action:  2\n",
            "score= 0.0\n",
            "621426\n",
            "256246\n",
            "331241\n",
            "135433\n",
            "62\u001b[91m3\u001b[0m431\n",
            "Step 4\n",
            "Action:  2\n",
            "score= 0.0\n",
            "621426\n",
            "256246\n",
            "331241\n",
            "135433\n",
            "62\u001b[91m3\u001b[0m431\n",
            "Goal reached! reward= 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a245ytFhwGg3"
      },
      "source": [
        ""
      ],
      "execution_count": 129,
      "outputs": []
    }
  ]
}