{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"docker run -p 9000:8080 -p 6006:6006 asia-docker.pkg.dev/colab-images/public/runtime","metadata":{"id":"dFK9SV8FkPcC"}},{"cell_type":"code","source":"# Initialize\n!git clone --recursive https://github.com/mitosagi/puzzdra-nnsolver\n%cd /kaggle/working/puzzdra-nnsolver\n# %cd /content/puzzdra-nnsolver\n!pip install --log=pip_log -e .\n!pip install torchsummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"84RFF9fTkEev","outputId":"26a3c409-01f6-4ab6-f28f-63148d74d839","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-11-04T01:29:58.329703Z","iopub.execute_input":"2023-11-04T01:29:58.329973Z","iopub.status.idle":"2023-11-04T01:30:39.114311Z","shell.execute_reply.started":"2023-11-04T01:29:58.329950Z","shell.execute_reply":"2023-11-04T01:30:39.113044Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'puzzdra-nnsolver'...\nremote: Enumerating objects: 302, done.\u001b[K\nremote: Counting objects: 100% (57/57), done.\u001b[K\nremote: Compressing objects: 100% (57/57), done.\u001b[K\nremote: Total 302 (delta 33), reused 0 (delta 0), pack-reused 245\u001b[K\nReceiving objects: 100% (302/302), 8.31 MiB | 20.17 MiB/s, done.\nResolving deltas: 100% (175/175), done.\nSubmodule 'extern/pybind11' (https://github.com/pybind/pybind11) registered for path 'extern/pybind11'\nCloning into '/kaggle/working/puzzdra-nnsolver/extern/pybind11'...\nremote: Enumerating objects: 27236, done.        \nremote: Counting objects: 100% (8/8), done.        \nremote: Compressing objects: 100% (6/6), done.        \nremote: Total 27236 (delta 1), reused 4 (delta 1), pack-reused 27228        \nReceiving objects: 100% (27236/27236), 10.58 MiB | 21.19 MiB/s, done.\nResolving deltas: 100% (19170/19170), done.\nSubmodule path 'extern/pybind11': checked out '8de7772cc72daca8e947b79b83fea46214931604'\n/kaggle/working/puzzdra-nnsolver\nObtaining file:///kaggle/working/puzzdra-nnsolver\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nInstalling collected packages: Puzzpy\n  Running setup.py develop for Puzzpy\nSuccessfully installed Puzzpy-1.0\nCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport numpy as np\nfrom puzzpy import PuzzTable\n\ndrop_color = 3\nboard_width = 6\nboard_height = 5\n\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    BLACK = '\\033[30m'\n    RED = '\\033[31m'\n    GREEN = '\\033[32m'\n    YELLOW = '\\033[33m'\n    BLUE = '\\033[34m'\n    MAGENTA = '\\033[35m'\n    CYAN = '\\033[36m'\n\ndef npUint8(array):\n    return np.array(array, dtype=np.uint8)\n\ndef np_float(array):\n    return np.array(array, dtype=np.float32)\n\nclass PuzzBoard():\n    def reset(self):\n        self.prev_action = 255\n\n        while True:\n            self.table = PuzzTable(\"\".join([str(random.randrange(drop_color)) for i in range(board_width*board_height)]), random.randrange(board_width), random.randrange(board_height), 50) # n色陣　操作時間m秒\n            if self.table.eval_otoshi() == 0:\n                break\n\n        return npUint8(self.table.get_table())\n\n    def step(self, peek = False, force_action = None):\n        next_tables = self.table.next_tables()\n        valid_actions = [action for action, table in enumerate(next_tables) if table.get_table()[0][0] != 127 and abs(action - self.prev_action) != 2]\n                    \n        if not peek:\n            if force_action != None:\n                if force_action in valid_actions:\n                    self.prev_action = force_action\n                    print(\"action success\")\n                else:\n                    self.prev_action = random.choice(valid_actions)\n                    print(\"action fail\")\n            else:\n                self.prev_action = random.choice(valid_actions)\n            self.table = next_tables[self.prev_action]\n\n        return self.prev_action, npUint8([(npUint8(table.get_table()) if action in valid_actions else np.zeros_like(npUint8(table.get_table()))) for action, table in enumerate(next_tables)])\n    def render(self):\n        tcolor = [bcolors.RED, bcolors.BLUE, bcolors.GREEN, bcolors.MAGENTA, bcolors.YELLOW, bcolors.BLACK]\n        start = self.table.get_XY_as_table()\n        table = self.table.get_table()\n        for i in range(board_height):\n            for j in range(board_width):\n                if start[i][j] == 1:\n                    print(tcolor[table[i][j]-1]  +  bcolors.UNDERLINE + \"●\" + bcolors.ENDC, end='')\n                else:\n                    print(tcolor[table[i][j]-1]  + \"●\" + bcolors.ENDC, end='')\n            print('')","metadata":{"id":"8nSDWSShkEex","execution":{"iopub.status.busy":"2023-11-04T01:41:36.614522Z","iopub.execute_input":"2023-11-04T01:41:36.614922Z","iopub.status.idle":"2023-11-04T01:41:36.631684Z","shell.execute_reply.started":"2023-11-04T01:41:36.614893Z","shell.execute_reply":"2023-11-04T01:41:36.630504Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"env = PuzzBoard()\nobs = env.reset()\nenv.render()\n\nn_steps = 3\n\nfor step in range(n_steps):\n    print(\"Step {}\".format(step + 1))\n    action, obs = env.step()\n    with np.printoptions(threshold=np.inf):\n        print(\"action: \", action)\n#         print(obs)\n        env.render()","metadata":{"id":"j-LdMczrkEex","outputId":"a70b54c3-159b-4cf0-8236-7dbe780189dd","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-11-04T01:30:39.140114Z","iopub.execute_input":"2023-11-04T01:30:39.140714Z","iopub.status.idle":"2023-11-04T01:30:39.152175Z","shell.execute_reply.started":"2023-11-04T01:30:39.140685Z","shell.execute_reply":"2023-11-04T01:30:39.151212Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m\u001b[4m●\u001b[0m\nStep 1\naction:  2\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m\u001b[4m●\u001b[0m\u001b[34m●\u001b[0m\nStep 2\naction:  2\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m\u001b[4m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\nStep 3\naction:  2\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\n\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m\u001b[4m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def make_data(num_step, env):\n    start = env.reset()\n\n    sample_step = random.randrange(1, num_step + 1)\n\n    for i in range(sample_step):\n        answer, obs = env.step()\n    sample_obs = obs\n\n    for i in range(num_step - sample_step):\n        answer, obs = env.step()\n    end = obs[answer]\n\n    return np.concatenate([npUint8([start]),sample_obs,npUint8([end])]), answer\ndef make_data_fast(length):\n    env = PuzzBoard()\n    return [make_data(50, env) for i in range(length)]\n\n# from multiprocessing import Pool\n# process = 10\n# p = Pool(process)\n# %time result = p.map(make_data_fast, [1_000_000 // process  for i in range(process)]) # 100万データ生成に5分22秒かかる\n\n# import itertools\n# sample_data = npUint8([sample[0] for sample in itertools.chain(*result)])\n# sample_labels = npUint8([sample[1] for sample in itertools.chain(*result)])\n# np.save('sample_data_50', sample_data)\n# np.save('sample_labels_50', sample_labels)","metadata":{"id":"HO8nP6WCkEex","outputId":"8e9dff77-fea4-43cf-d966-6450a970f28f","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-11-04T01:30:39.154248Z","iopub.execute_input":"2023-11-04T01:30:39.154556Z","iopub.status.idle":"2023-11-04T01:30:39.162797Z","shell.execute_reply.started":"2023-11-04T01:30:39.154529Z","shell.execute_reply":"2023-11-04T01:30:39.161934Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sample_data = np.load('/kaggle/input/puzz-50/sample_data_50.npy')\nsample_labels = np.load('/kaggle/input/puzz-50/sample_labels_50.npy')\nfrom torch.utils.data import Dataset\nclass PuzzDataSet(Dataset):\n    def __init__(self, x, y):\n        self.data = np.array(x, dtype=np.float32)\n        self.labels = np.array(np.identity(4)[y], dtype=np.float32) # 4 for actions\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\ndataset = PuzzDataSet(sample_data[0:len(sample_data) - 1_000],sample_labels[0:len(sample_labels) - 1_000])\nprint(\"dataset: \", dataset.__len__())\nprint(\"dataset: \", dataset.__getitem__(1))\ntestset = PuzzDataSet(sample_data[len(sample_data) - 1_000:],sample_labels[len(sample_labels) - 1_000:])\nprint(\"testset: \", testset.__len__())","metadata":{"id":"d1V18BD9kEey","outputId":"4796543f-1d39-40df-9bfd-7b5a887a408a","execution":{"iopub.status.busy":"2023-11-04T01:30:39.164206Z","iopub.execute_input":"2023-11-04T01:30:39.164639Z","iopub.status.idle":"2023-11-04T01:30:43.384229Z","shell.execute_reply.started":"2023-11-04T01:30:39.164606Z","shell.execute_reply":"2023-11-04T01:30:43.383199Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"dataset:  999000\ndataset:  (array([[[3., 2., 3., 1., 2., 3.],\n        [2., 3., 2., 1., 2., 3.],\n        [2., 1., 2., 2., 3., 2.],\n        [3., 2., 3., 3., 2., 1.],\n        [2., 3., 1., 2., 2., 1.]],\n\n       [[2., 3., 2., 1., 2., 3.],\n        [2., 3., 2., 1., 2., 3.],\n        [1., 3., 2., 2., 3., 2.],\n        [3., 2., 3., 3., 2., 1.],\n        [2., 3., 1., 2., 2., 1.]],\n\n       [[2., 3., 2., 1., 2., 3.],\n        [2., 3., 2., 1., 2., 3.],\n        [3., 1., 2., 2., 3., 2.],\n        [3., 2., 3., 3., 2., 1.],\n        [2., 3., 1., 2., 2., 1.]],\n\n       [[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]],\n\n       [[2., 3., 2., 2., 3., 3.],\n        [2., 3., 2., 1., 2., 2.],\n        [3., 2., 2., 2., 3., 1.],\n        [2., 1., 2., 2., 1., 1.],\n        [3., 1., 3., 3., 3., 2.]]], dtype=float32), array([0., 0., 1., 0.], dtype=float32))\ntestset:  1000\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchsummary import summary\nfrom torchvision.models import resnet18, ResNet18_Weights\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        num_input_channel = 6\n        num_classes = 4\n        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n        resnet = resnet18()\n        resnet.conv1 = nn.Conv2d(num_input_channel, 64, kernel_size=7, stride=2, padding=3,bias=False)\n        resnet.fc = nn.Linear(512, num_classes)\n        self.resnet = resnet\n    def forward(self, x):\n        x = F.interpolate(x, size=None, scale_factor=2, mode='nearest')\n        x = self.resnet(x)\n        return x\n\nmodel = Net()\nfor i, param in enumerate(model.parameters()):\n#     param.requires_grad = False if len(param) != 512 and len(param) != 4 else True\n    param.requires_grad = True if i >= 0 else False\n    print(len(param), param.requires_grad)\n\n# model = CNN()\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(\"device: \", device)\nmodel = model.to(device)\nprint(summary(model, (6, 5, 6)))\n\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=200, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=200, shuffle=True)\n\ncriterion = nn.CrossEntropyLoss()\n# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.SGD(model.parameters(), lr=0.0005)\n\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\n\nimport datetime\n\ndef train(epoch):\n    total_loss = 0\n    total_size = 0\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        total_loss += loss.item()\n        total_size += data.size(0)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            now = datetime.datetime.now()\n            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n                now,\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), total_loss / total_size))\n            writer.add_scalar('Loss/train', total_loss / total_size, epoch)\n            test(epoch)\ndef test(epoch):\n    total_loss = 0\n    total_size = 0\n    model.eval()\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        loss = criterion(output, target)\n        total_loss += loss.item()\n        total_size += data.size(0)\n        if batch_idx % 100 == 0:\n            now = datetime.datetime.now()\n            print('[{}] Test Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}'.format(\n                now,\n                epoch, batch_idx * len(data), len(test_loader.dataset),\n                100. * batch_idx / len(test_loader), total_loss / total_size))\n            writer.add_scalar('Loss/test', total_loss / total_size, epoch)\n            break\n\nfor epoch in range(1, 10 + 1):\n    train(epoch)\n    test(epoch)","metadata":{"id":"JyXo_fAOkEey","outputId":"f789fc07-ffbb-4cdf-89ba-bc38d386830d","execution":{"iopub.status.busy":"2023-11-04T01:46:56.348433Z","iopub.execute_input":"2023-11-04T01:46:56.349419Z","iopub.status.idle":"2023-11-04T01:53:59.864643Z","shell.execute_reply.started":"2023-11-04T01:46:56.349378Z","shell.execute_reply":"2023-11-04T01:53:59.863500Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n64 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n128 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n256 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n512 True\n4 True\n4 True\ndevice:  cuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1             [-1, 64, 5, 6]          18,816\n       BatchNorm2d-2             [-1, 64, 5, 6]             128\n              ReLU-3             [-1, 64, 5, 6]               0\n         MaxPool2d-4             [-1, 64, 3, 3]               0\n            Conv2d-5             [-1, 64, 3, 3]          36,864\n       BatchNorm2d-6             [-1, 64, 3, 3]             128\n              ReLU-7             [-1, 64, 3, 3]               0\n            Conv2d-8             [-1, 64, 3, 3]          36,864\n       BatchNorm2d-9             [-1, 64, 3, 3]             128\n             ReLU-10             [-1, 64, 3, 3]               0\n       BasicBlock-11             [-1, 64, 3, 3]               0\n           Conv2d-12             [-1, 64, 3, 3]          36,864\n      BatchNorm2d-13             [-1, 64, 3, 3]             128\n             ReLU-14             [-1, 64, 3, 3]               0\n           Conv2d-15             [-1, 64, 3, 3]          36,864\n      BatchNorm2d-16             [-1, 64, 3, 3]             128\n             ReLU-17             [-1, 64, 3, 3]               0\n       BasicBlock-18             [-1, 64, 3, 3]               0\n           Conv2d-19            [-1, 128, 2, 2]          73,728\n      BatchNorm2d-20            [-1, 128, 2, 2]             256\n             ReLU-21            [-1, 128, 2, 2]               0\n           Conv2d-22            [-1, 128, 2, 2]         147,456\n      BatchNorm2d-23            [-1, 128, 2, 2]             256\n           Conv2d-24            [-1, 128, 2, 2]           8,192\n      BatchNorm2d-25            [-1, 128, 2, 2]             256\n             ReLU-26            [-1, 128, 2, 2]               0\n       BasicBlock-27            [-1, 128, 2, 2]               0\n           Conv2d-28            [-1, 128, 2, 2]         147,456\n      BatchNorm2d-29            [-1, 128, 2, 2]             256\n             ReLU-30            [-1, 128, 2, 2]               0\n           Conv2d-31            [-1, 128, 2, 2]         147,456\n      BatchNorm2d-32            [-1, 128, 2, 2]             256\n             ReLU-33            [-1, 128, 2, 2]               0\n       BasicBlock-34            [-1, 128, 2, 2]               0\n           Conv2d-35            [-1, 256, 1, 1]         294,912\n      BatchNorm2d-36            [-1, 256, 1, 1]             512\n             ReLU-37            [-1, 256, 1, 1]               0\n           Conv2d-38            [-1, 256, 1, 1]         589,824\n      BatchNorm2d-39            [-1, 256, 1, 1]             512\n           Conv2d-40            [-1, 256, 1, 1]          32,768\n      BatchNorm2d-41            [-1, 256, 1, 1]             512\n             ReLU-42            [-1, 256, 1, 1]               0\n       BasicBlock-43            [-1, 256, 1, 1]               0\n           Conv2d-44            [-1, 256, 1, 1]         589,824\n      BatchNorm2d-45            [-1, 256, 1, 1]             512\n             ReLU-46            [-1, 256, 1, 1]               0\n           Conv2d-47            [-1, 256, 1, 1]         589,824\n      BatchNorm2d-48            [-1, 256, 1, 1]             512\n             ReLU-49            [-1, 256, 1, 1]               0\n       BasicBlock-50            [-1, 256, 1, 1]               0\n           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n             ReLU-53            [-1, 512, 1, 1]               0\n           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n           Conv2d-56            [-1, 512, 1, 1]         131,072\n      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n             ReLU-58            [-1, 512, 1, 1]               0\n       BasicBlock-59            [-1, 512, 1, 1]               0\n           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n             ReLU-62            [-1, 512, 1, 1]               0\n           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n             ReLU-65            [-1, 512, 1, 1]               0\n       BasicBlock-66            [-1, 512, 1, 1]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                    [-1, 4]           2,052\n           ResNet-69                    [-1, 4]               0\n================================================================\nTotal params: 11,187,972\nTrainable params: 11,187,972\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.27\nParams size (MB): 42.68\nEstimated Total Size (MB): 42.95\n----------------------------------------------------------------\nNone\n[2023-11-04 01:46:56.851312] Train Epoch: 1 [0/999000 (0%)]\tAverage loss: 0.007386\n[2023-11-04 01:46:56.935913] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006999\n[2023-11-04 01:47:06.194303] Train Epoch: 1 [20000/999000 (2%)]\tAverage loss: 0.006958\n[2023-11-04 01:47:06.283244] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006968\n[2023-11-04 01:47:15.538454] Train Epoch: 1 [40000/999000 (4%)]\tAverage loss: 0.006949\n[2023-11-04 01:47:15.627415] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006943\n[2023-11-04 01:47:24.889876] Train Epoch: 1 [60000/999000 (6%)]\tAverage loss: 0.006947\n[2023-11-04 01:47:24.978826] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006963\n[2023-11-04 01:47:34.236597] Train Epoch: 1 [80000/999000 (8%)]\tAverage loss: 0.006944\n[2023-11-04 01:47:34.325530] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006929\n[2023-11-04 01:47:43.574877] Train Epoch: 1 [100000/999000 (10%)]\tAverage loss: 0.006943\n[2023-11-04 01:47:43.663600] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006924\n[2023-11-04 01:47:52.927814] Train Epoch: 1 [120000/999000 (12%)]\tAverage loss: 0.006943\n[2023-11-04 01:47:53.014518] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006915\n[2023-11-04 01:48:02.277775] Train Epoch: 1 [140000/999000 (14%)]\tAverage loss: 0.006942\n[2023-11-04 01:48:02.366674] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006936\n[2023-11-04 01:48:11.632553] Train Epoch: 1 [160000/999000 (16%)]\tAverage loss: 0.006941\n[2023-11-04 01:48:11.721535] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006945\n[2023-11-04 01:48:20.992511] Train Epoch: 1 [180000/999000 (18%)]\tAverage loss: 0.006941\n[2023-11-04 01:48:21.080862] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006945\n[2023-11-04 01:48:30.340535] Train Epoch: 1 [200000/999000 (20%)]\tAverage loss: 0.006940\n[2023-11-04 01:48:30.429710] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006924\n[2023-11-04 01:48:39.693751] Train Epoch: 1 [220000/999000 (22%)]\tAverage loss: 0.006940\n[2023-11-04 01:48:39.782548] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006953\n[2023-11-04 01:48:49.050351] Train Epoch: 1 [240000/999000 (24%)]\tAverage loss: 0.006939\n[2023-11-04 01:48:49.139177] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006931\n[2023-11-04 01:48:58.401699] Train Epoch: 1 [260000/999000 (26%)]\tAverage loss: 0.006939\n[2023-11-04 01:48:58.491223] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006935\n[2023-11-04 01:49:07.749432] Train Epoch: 1 [280000/999000 (28%)]\tAverage loss: 0.006938\n[2023-11-04 01:49:07.838380] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006953\n[2023-11-04 01:49:17.102598] Train Epoch: 1 [300000/999000 (30%)]\tAverage loss: 0.006938\n[2023-11-04 01:49:17.191664] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006945\n[2023-11-04 01:49:26.458997] Train Epoch: 1 [320000/999000 (32%)]\tAverage loss: 0.006938\n[2023-11-04 01:49:26.545994] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006919\n[2023-11-04 01:49:35.808233] Train Epoch: 1 [340000/999000 (34%)]\tAverage loss: 0.006938\n[2023-11-04 01:49:35.897243] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006941\n[2023-11-04 01:49:45.156190] Train Epoch: 1 [360000/999000 (36%)]\tAverage loss: 0.006938\n[2023-11-04 01:49:45.245025] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006957\n[2023-11-04 01:49:54.506750] Train Epoch: 1 [380000/999000 (38%)]\tAverage loss: 0.006938\n[2023-11-04 01:49:54.595498] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006937\n[2023-11-04 01:50:03.858277] Train Epoch: 1 [400000/999000 (40%)]\tAverage loss: 0.006938\n[2023-11-04 01:50:03.947108] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006944\n[2023-11-04 01:50:13.205810] Train Epoch: 1 [420000/999000 (42%)]\tAverage loss: 0.006937\n[2023-11-04 01:50:13.294811] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006901\n[2023-11-04 01:50:22.555294] Train Epoch: 1 [440000/999000 (44%)]\tAverage loss: 0.006937\n[2023-11-04 01:50:22.644162] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006939\n[2023-11-04 01:50:31.914142] Train Epoch: 1 [460000/999000 (46%)]\tAverage loss: 0.006937\n[2023-11-04 01:50:32.003093] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006923\n[2023-11-04 01:50:41.261245] Train Epoch: 1 [480000/999000 (48%)]\tAverage loss: 0.006937\n[2023-11-04 01:50:41.350356] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006921\n[2023-11-04 01:50:50.609412] Train Epoch: 1 [500000/999000 (50%)]\tAverage loss: 0.006937\n[2023-11-04 01:50:50.698556] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006929\n[2023-11-04 01:50:59.957840] Train Epoch: 1 [520000/999000 (52%)]\tAverage loss: 0.006937\n[2023-11-04 01:51:00.047021] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006923\n[2023-11-04 01:51:09.310552] Train Epoch: 1 [540000/999000 (54%)]\tAverage loss: 0.006936\n[2023-11-04 01:51:09.399270] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006940\n[2023-11-04 01:51:18.660543] Train Epoch: 1 [560000/999000 (56%)]\tAverage loss: 0.006936\n[2023-11-04 01:51:18.749509] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006909\n[2023-11-04 01:51:28.000857] Train Epoch: 1 [580000/999000 (58%)]\tAverage loss: 0.006936\n[2023-11-04 01:51:28.089812] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006916\n[2023-11-04 01:51:37.348012] Train Epoch: 1 [600000/999000 (60%)]\tAverage loss: 0.006936\n[2023-11-04 01:51:37.437206] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006921\n[2023-11-04 01:51:46.701852] Train Epoch: 1 [620000/999000 (62%)]\tAverage loss: 0.006936\n[2023-11-04 01:51:46.790776] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006943\n[2023-11-04 01:51:56.030397] Train Epoch: 1 [640000/999000 (64%)]\tAverage loss: 0.006936\n[2023-11-04 01:51:56.118983] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006892\n[2023-11-04 01:52:05.382708] Train Epoch: 1 [660000/999000 (66%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:05.471692] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006937\n[2023-11-04 01:52:14.732576] Train Epoch: 1 [680000/999000 (68%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:14.821151] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006931\n[2023-11-04 01:52:24.076061] Train Epoch: 1 [700000/999000 (70%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:24.165013] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006924\n[2023-11-04 01:52:33.420332] Train Epoch: 1 [720000/999000 (72%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:33.509311] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006947\n[2023-11-04 01:52:42.775113] Train Epoch: 1 [740000/999000 (74%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:42.863954] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:52.132189] Train Epoch: 1 [760000/999000 (76%)]\tAverage loss: 0.006936\n[2023-11-04 01:52:52.221270] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006913\n[2023-11-04 01:53:01.487927] Train Epoch: 1 [780000/999000 (78%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:01.576890] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006963\n[2023-11-04 01:53:10.845802] Train Epoch: 1 [800000/999000 (80%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:10.934637] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006938\n[2023-11-04 01:53:20.198191] Train Epoch: 1 [820000/999000 (82%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:20.287280] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006918\n[2023-11-04 01:53:29.554941] Train Epoch: 1 [840000/999000 (84%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:29.643852] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006941\n[2023-11-04 01:53:38.909501] Train Epoch: 1 [860000/999000 (86%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:38.998559] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006952\n[2023-11-04 01:53:48.257406] Train Epoch: 1 [880000/999000 (88%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:48.346178] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006940\n[2023-11-04 01:53:57.608975] Train Epoch: 1 [900000/999000 (90%)]\tAverage loss: 0.006935\n[2023-11-04 01:53:57.697977] Test Epoch: 1 [0/1000 (0%)]\tAverage loss: 0.006904\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     test(epoch)\n","Cell \u001b[0;32mIn[25], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 52\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     54\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs","metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:01:09.518771Z","iopub.status.idle":"2023-11-03T23:01:09.519220Z","shell.execute_reply.started":"2023-11-03T23:01:09.518988Z","shell.execute_reply":"2023-11-03T23:01:09.519011Z"},"id":"XQhVi-JKkEey","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model_puzz_ressnet.pth')","metadata":{"execution":{"iopub.status.busy":"2023-11-04T01:07:24.006380Z","iopub.execute_input":"2023-11-04T01:07:24.007199Z","iopub.status.idle":"2023-11-04T01:07:24.103534Z","shell.execute_reply.started":"2023-11-04T01:07:24.007161Z","shell.execute_reply":"2023-11-04T01:07:24.102707Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = torch.load('/kaggle/input/puzz-resnet-weight/model_puzz_ressnet.pth', map_location=torch.device('cpu'))","metadata":{"execution":{"iopub.status.busy":"2023-11-04T01:11:08.396357Z","iopub.execute_input":"2023-11-04T01:11:08.396773Z","iopub.status.idle":"2023-11-04T01:11:08.837344Z","shell.execute_reply.started":"2023-11-04T01:11:08.396742Z","shell.execute_reply":"2023-11-04T01:11:08.836039Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from scipy.special import softmax\nmodel.eval()\n\ninfer_board = PuzzBoard()\ntmp_board = infer_board.reset().ravel()\nstart = tmp_board.reshape([5,6])\nend = np.sort(tmp_board).reshape([5,6])\nprint(start)\nprint(end)\nfor i in range(1000):\n     \n    data = np_float([np_float(np.concatenate([npUint8([start]),infer_board.step(peek=True)[1],npUint8([end])]))])\n    action = model(torch.from_numpy(data).to(device)).tolist()[0]\n#     print(softmax(action))\n#     print(action.index(max(action)))\n    infer_board.step(force_action = action.index(max(action)))\n    \nprint(\"step: \", i)\ninfer_board.render()","metadata":{"id":"JcpS0jdVkEey","execution":{"iopub.status.busy":"2023-11-04T01:42:04.129484Z","iopub.execute_input":"2023-11-04T01:42:04.129860Z","iopub.status.idle":"2023-11-04T01:42:07.077720Z","shell.execute_reply.started":"2023-11-04T01:42:04.129830Z","shell.execute_reply":"2023-11-04T01:42:07.076831Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[[2 3 2 1 3 2]\n [3 2 3 2 3 1]\n [3 3 1 2 1 2]\n [2 2 3 1 3 3]\n [3 1 1 2 2 1]]\n[[1 1 1 1 1 1]\n [1 1 2 2 2 2]\n [2 2 2 2 2 2]\n [2 3 3 3 3 3]\n [3 3 3 3 3 3]]\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction fail\naction fail\naction fail\naction fail\naction success\naction fail\naction fail\naction fail\naction success\naction fail\naction fail\naction fail\naction fail\naction fail\naction fail\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction success\naction fail\naction fail\naction fail\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction fail\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction fail\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction success\naction fail\naction success\naction fail\naction fail\naction success\naction fail\naction fail\naction fail\naction success\naction success\naction fail\naction fail\naction success\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction success\naction fail\naction fail\naction success\naction success\naction success\naction success\naction fail\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\naction success\nstep:  999\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\n\u001b[32m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[31m●\u001b[0m\u001b[34m●\u001b[0m\u001b[34m\u001b[4m●\u001b[0m\n\u001b[34m●\u001b[0m\u001b[34m●\u001b[0m\u001b[31m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\u001b[32m●\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}